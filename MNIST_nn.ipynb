{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "86CxkMLZdQoY"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from torch.utils.data import random_split, DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.transforms import ToTensor"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# I needed to run this on T4 GPU to work\n",
        "torch.randn(5).cuda()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p9_TEIqjoZ6b",
        "outputId": "dedbbcfc-9011-403c-a163-a5d7bc9efcb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 1.0783, -1.1575,  0.0319,  0.0214,  0.1593], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train, determine the split between training and validation\n",
        "train_data = datasets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
        "# test_data = dataset.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
        "\n",
        "train, val = random_split(train_data, [55000, 5000])\n",
        "train_loader = DataLoader(train, batch_size=32)\n",
        "val_loader = DataLoader(val, batch_size=32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "bw7Ysr9XiQM0",
        "outputId": "8abf668d-1aba-42ad-a17f-170c4ff15e82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 17.8MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 504kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 4.50MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 5.12MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define model\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(28 * 28, 64),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(64, 64),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(64, 64),\n",
        "    # Add the stuff below for a 'fancy' model\n",
        "    nn.Dropout(0.1), # if we are overfitting, helps us treat this\n",
        "    nn.Linear(64, 10) # the ending layer should manifest into 10 neurons\n",
        ")"
      ],
      "metadata": {
        "id": "_7-CMolreOqZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Residuals can create a direct connection between a layer x and a layer x + c by setting certain biases to negative numbers\n",
        "\n",
        "Residuals are useful because they allow the model to extract more information from the original data\n",
        "\n",
        "Residuals treat the vanishing gradient problem, which is when the gradient changes become really small\n",
        "\n",
        "So, residuals basically help us create deeper ML models"
      ],
      "metadata": {
        "id": "KazCuY-l2ex6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a more flexible model using residuals\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.l1 = nn.Linear(28 * 28, 64)\n",
        "        self.l2 = nn.Linear(64, 64)\n",
        "        self.l3 = nn.Linear(64, 10)\n",
        "        self.do = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h1 = nn.functional.relu(self.l1(x))\n",
        "        h2 = nn.functional.relu(self.l2(h1))\n",
        "        do = self.do(h2 + h1) # highway networks\n",
        "        logits = self.l3(do)\n",
        "        return logits\n",
        "\n",
        "model = ResNet().cuda() # model is generated, then moved to GPU"
      ],
      "metadata": {
        "id": "mguyaz_4vxdt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define optimizer\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = 1e-2)"
      ],
      "metadata": {
        "id": "NW6x_mRkeyGL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define loss as an objective function\n",
        "loss = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "SWLNh3tAe8Gl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training and validation loops\n",
        "nb_epochs = 5\n",
        "for epoch in range(nb_epochs): # full pass through the dataset\n",
        "    losses = list() # initialize empty list for which to store the last 10 losses\n",
        "    accuracies = list()\n",
        "    model.train() # put the model in training mode (this line is needed because we used dropout to make our model)\n",
        "\n",
        "    for batch in train_loader:\n",
        "        x, y = batch\n",
        "\n",
        "        # x is b x 1 x 28 x 28 (need to reshape it)\n",
        "        b = x.size(0) # flatten it from an image format\n",
        "        x = x.view(b, -1).cuda() # the -1 for some reason means (28x28)\n",
        "\n",
        "        # Forward; computing objective function\n",
        "        l = loss(model(x), y.cuda()) # since the model is already in the GPU, do the processing in the same piece of hardware\n",
        "\n",
        "        # Clean up the gradients (not always necessary), but we want to clear them for now\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Acculumate the partial derivatives of the loss wrt params\n",
        "        l.backward() # note that we are ACCUMULATING, not just SUMMING\n",
        "\n",
        "        # Step in the opposite direction of the gradient (why opposite?)\n",
        "        optimizer.step()\n",
        "\n",
        "    # We need to see what the loss is doing though\n",
        "    # We are otherwise bleeding memory in the program because l is big\n",
        "\n",
        "        losses.append(l.item())\n",
        "        accuracies.append(y.eq(model(x).detach().argmax(dim=1).cpu()).float().mean())\n",
        "\n",
        "    print(f'Epoch {epoch + 1}', end = ', ')\n",
        "    print(f'Training Loss: {torch.tensor(losses).mean():.2f}', end = ', ')\n",
        "    print(f'Training Accuracy: {torch.tensor(accuracies).mean():.2f}')\n",
        "\n",
        "    # Validation\n",
        "    model.eval() # put the model in validation mode (this line is needed because we used dropout to make our model)\n",
        "    losses = list() # initialize empty list for which to store the last 10 losses\n",
        "    accuracies = list()\n",
        "\n",
        "    for batch in val_loader:\n",
        "        x, y = batch\n",
        "\n",
        "        # x is b x 1 x 28 x 28 (need to reshape it)\n",
        "        b = x.size(0) # flatten it from an image format\n",
        "        x = x.view(b, -1).cuda() # the -1 for some reason means (28x28)\n",
        "\n",
        "        # Forward (disable gradient computation for validation)\n",
        "        with torch.no_grad():\n",
        "            l = loss(model(x), y.cuda())\n",
        "\n",
        "        # Printing out the loss\n",
        "        losses.append(l.item())\n",
        "        accuracies.append(y.eq(model(x).detach().argmax(dim=1).cpu()).float().mean())\n",
        "\n",
        "    print(f'Epoch {epoch + 1}', end = ', ')\n",
        "    print(f'Validation Loss: {torch.tensor(losses).mean():.2f}', end = ', ')\n",
        "    print(f'Validation Accuracy: {torch.tensor(accuracies).mean():.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G5xu0Z8ZfG6A",
        "outputId": "4866db5a-e7bd-4c5f-c3ac-74b7aac5f05b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Training Loss: 0.21, Training Accuracy: 0.94\n",
            "Epoch 1, Validation Loss: 0.20, Validation Accuracy: 0.94\n",
            "Epoch 2, Training Loss: 0.19, Training Accuracy: 0.95\n",
            "Epoch 2, Validation Loss: 0.19, Validation Accuracy: 0.95\n",
            "Epoch 3, Training Loss: 0.18, Training Accuracy: 0.95\n",
            "Epoch 3, Validation Loss: 0.17, Validation Accuracy: 0.95\n",
            "Epoch 4, Training Loss: 0.16, Training Accuracy: 0.96\n",
            "Epoch 4, Validation Loss: 0.16, Validation Accuracy: 0.95\n",
            "Epoch 5, Training Loss: 0.15, Training Accuracy: 0.96\n",
            "Epoch 5, Validation Loss: 0.15, Validation Accuracy: 0.96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explanation of Code (From Perplexity)\n",
        "\n",
        "**Overall Goal:**\n",
        "\n",
        "The code trains a neural network to classify handwritten digits from the MNIST dataset. MNIST contains images of digits 0 through 9. The network learns to take an image as input and output a prediction of which digit it represents.\n",
        "\n",
        "**Key Components:**\n",
        "\n",
        "1.  **Data Loading and Preprocessing:**\n",
        "    *   The code uses `torchvision.datasets.MNIST` to download and load the MNIST dataset.\n",
        "    *   It splits the training data into a training set (55,000 images) and a validation set (5,000 images).  The validation set is used to monitor the model's performance on unseen data during training, helping to prevent overfitting.\n",
        "    *   `DataLoader` is used to create batches of data.  This is more efficient than processing images one at a time.\n",
        "\n",
        "2.  **Model Definition (ResNet):**\n",
        "    *   The code defines a `ResNet` class, which is a type of neural network architecture that uses residual connections.\n",
        "    *   **Layers:** The `ResNet` consists of linear layers (`nn.Linear`), ReLU activation functions (`nn.functional.relu`), and a dropout layer (`nn.Dropout`).\n",
        "        *   The first linear layer (`l1`) takes the flattened image (28x28 pixels = 784 inputs) and transforms it to 64 features.\n",
        "        *   The second linear layer (`l2`) transforms the 64 features to another 64 features.\n",
        "        *   The third linear layer (`l3`) transforms the 64 features to 10 outputs (one for each digit 0-9).  These outputs are often called \"logits\".\n",
        "    *   **Residual Connection:** The line `do = self.do(h2 + h1)` is the key part.  It adds the output of the first layer (`h1`) to the output of the second layer (`h2`) *before* applying dropout (`do`).  This creates a shortcut connection (a \"highway\") that allows information from earlier layers to flow more easily to later layers.\n",
        "\n",
        "3.  **Why Residuals?**\n",
        "    *   **Vanishing Gradients:** Residual connections help mitigate the vanishing gradient problem, which can occur in deep networks.  When gradients become very small, it becomes difficult for earlier layers to learn. Residuals provide an alternate path for gradients to flow.\n",
        "    *   **Easier Optimization:** Residual connections can make the optimization landscape (the \"shape\" of the loss function) smoother, making it easier for the optimizer to find a good solution.\n",
        "    *   **Deeper Models:** They allow you to train deeper networks more effectively.\n",
        "\n",
        "4.  **Loss Function and Optimizer:**\n",
        "    *   **Loss:** `nn.CrossEntropyLoss()` is the loss function.  It measures the difference between the model's predicted probabilities and the true labels.\n",
        "    *   **Optimizer:** `torch.optim.SGD` is the optimizer (Stochastic Gradient Descent).  It's responsible for updating the model's parameters (weights and biases) to minimize the loss function. The learning rate (`lr=1e-2`) controls the step size during optimization.\n",
        "\n",
        "5.  **Training Loop:**\n",
        "    *   The code iterates through the training data for a specified number of epochs (`nb_epochs = 5`).\n",
        "    *   **Forward Pass:** For each batch of images:\n",
        "        *   The input images (`x`) are reshaped (flattened into a vector).\n",
        "        *   The images are moved to the GPU (using `.cuda()`).\n",
        "        *   The model makes a prediction (outputs \"logits\").\n",
        "        *   The loss is calculated.\n",
        "    *   **Backward Pass (Backpropagation):**\n",
        "        *   `optimizer.zero_grad()` clears the gradients from the previous iteration.\n",
        "        *   `l.backward()` calculates the gradients of the loss function with respect to the model's parameters.\n",
        "        *   `optimizer.step()` updates the model's parameters using the calculated gradients.\n",
        "    *   **Validation:** After each epoch, the model is evaluated on the validation set *without* updating the parameters. This gives an estimate of how well the model generalizes to unseen data.\n",
        "\n",
        "6.  **Evaluation:**\n",
        "    *   During validation, `torch.no_grad()` is used to disable gradient calculations, which saves memory and speeds up the process.\n",
        "    *   The code calculates the validation loss and accuracy.\n",
        "\n",
        "7.  **Output:**\n",
        "    *   The code prints the training loss/accuracy and validation loss/accuracy for each epoch. This allows you to monitor the training process and see if the model is improving.\n",
        "\n",
        "**In Summary:**\n",
        "\n",
        "This code implements a basic image classification task using a ResNet-like neural network in PyTorch. It loads the MNIST dataset, defines a neural network architecture with residual connections, trains the network using stochastic gradient descent, and evaluates its performance on a validation set. The goal is to learn a model that can accurately classify handwritten digits."
      ],
      "metadata": {
        "id": "Lk57a0ym5U6C"
      }
    }
  ]
}